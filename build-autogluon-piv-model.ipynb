{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('autogluon': conda)"
  },
  "interpreter": {
   "hash": "7cbda11db75a6a6f02552cb5b1d0c9db45260176a858c36f0b0a2fdf2092b644"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Build PIV classification model with Autogluon"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "source": [
    "## Prepare data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdkit_file = \"./data/piv_rdkit.csv\"\n",
    "flatring_file = \"./data/piv_flatring.csv\"\n",
    "fps_file = \"./data/piv_fps.csv\"\n",
    "rdkit_fps_file = \"./data/piv_rdkit_fps.csv\"\n",
    "flatring_fps_file = \"./data/piv_flatring_fps.csv\"\n",
    "flatring_rdkit_fps_file = \"./data/piv_flatring_rdkit_fps.csv\"\n",
    "\n",
    "# Select input file\n",
    "in_file = fps_file \n",
    "base = os.path.basename(in_file)\n",
    "in_name = os.path.splitext(base)[0]\n",
    "\n",
    "data = TabularDataset(data=in_file)\n",
    "df_train = data[data.Set == \"Train\"].copy()\n",
    "df_test = data[data.Set == \"Test\"].copy()\n",
    "\n",
    "df_train.drop(columns=[\"Set\", \"Rating\"], inplace=True)\n",
    "df_test.drop(columns=[\"Set\", \"Rating\"], inplace=True)\n",
    "\n",
    "print(\"train dataset\", df_train.shape)\n",
    "print(df_train.Value.value_counts())\n",
    "print(\"test dataset\", df_test.shape)\n",
    "print(df_test.Value.value_counts())"
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'Value'\n",
    "save_path = f\"./models/ag-binary-model-{in_name}\"\n",
    "id_columns = [\"Substance\", \"Canonical_Smiles\"]\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=label_column,\n",
    "    path=save_path,\n",
    "    problem_type='binary',\n",
    "    eval_metric='roc_auc',\n",
    "    learner_kwargs={\n",
    "        'positive_class': 1,\n",
    "        'ignored_columns': id_columns\n",
    "    },\n",
    ")\n",
    "predictor.fit(\n",
    "    train_data=df_train,\n",
    "    presets='best_quality',\n",
    "    #auto_stack=True,\n",
    "    verbosity=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predictor.fit_summary(verbosity=1)"
   ]
  },
  {
   "source": [
    "## Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'Value'\n",
    "\n",
    "# Evaluate model on test data\n",
    "y_pred = predictor.predict_proba(df_test.drop(columns=[label_column]))\n",
    "y_true = df_test[label_column]\n",
    "perf = predictor.evaluate_predictions(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    auxiliary_metrics=True,\n",
    "    silent=True,\n",
    "    detailed_report=True,\n",
    ")\n",
    "perf[\"dataset\"] = in_name\n",
    "perf\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion_matrix = perf[\"confusion_matrix\"]\n",
    "tp = df_confusion_matrix.iloc[0,0]\n",
    "fn = df_confusion_matrix.iloc[0,1]\n",
    "fp = df_confusion_matrix.iloc[1,0]\n",
    "tn = df_confusion_matrix.iloc[1,1]\n",
    "sensitivity = tp/(tp+fn)\n",
    "specifity   = tn/(tn+fp)\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "print(f\"Specifity  : {specifity}\")\n",
    "print(\"Confusion matrix\")\n",
    "(df_confusion_matrix.rename(columns={0: 'predicted positive',1: 'predicted negative'})\n",
    ".rename({0: 'observed positive',1: 'observed negative'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leaderboard = predictor.leaderboard(df_test, silent=True)\n",
    "df_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_importance = predictor.feature_importance(\n",
    "#     data=df_test,\n",
    "#     subsample_size=5000,\n",
    "#     num_shuffle_sets=10,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}